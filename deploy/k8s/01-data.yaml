apiVersion: v1
kind: Namespace
metadata:
  name: ongi-data
---
# -------------------------
# Redis (redis:7)
# -------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-conf
  namespace: ongi-data
data:
  redis.conf: |
    # 데이터 디렉토리 설정
    dir /data
    # RDB 자동 저장 조건 조정 (너무 자주 저장되는 것 방지)
    save 900 1
    save 300 100
    save 60 10000
    # AOF 활성화 (지속성 보장)
    appendonly yes
    appendfsync everysec
    auto-aof-rewrite-percentage 0
    # 메모리 정책: 메모리 초과 시 오래된 키 제거
    maxmemory-policy allkeys-lru
    # 쓰기 중단 방지 설정 (RDB 오류 시에도 계속 동작)
    stop-writes-on-bgsave-error no
    # 클라이언트 수 제한
    maxclients 10000
    # 백로그 큐 설정 (커널 값과 일치해야 함)
    tcp-backlog 1024
    # 로그 레벨
    loglevel notice
    bind 0.0.0.0
    protected-mode no
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: redis-data
  namespace: ongi-data
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 2Gi
  storageClassName: local-path
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: ongi-data
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
        - name: redis
          image: redis:7
          ports:
            - containerPort: 6379
          command: ["redis-server", "/usr/local/etc/redis/redis.conf"]
          volumeMounts:
            - name: redis-conf
              mountPath: /usr/local/etc/redis/redis.conf
              subPath: redis.conf
            - name: redis-data
              mountPath: /data
          resources:
            requests:
              cpu: "50m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
      volumes:
        - name: redis-conf
          configMap:
            name: redis-conf
        - name: redis-data
          persistentVolumeClaim:
            claimName: redis-data
---
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: ongi-data
spec:
  selector:
    app: redis
  ports:
    - name: redis
      port: 6379
      targetPort: 6379
---
# -------------------------
# Redis Exporter (oliver006/redis_exporter)
# -------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-exporter
  namespace: ongi-data
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis-exporter
  template:
    metadata:
      labels:
        app: redis-exporter
    spec:
      containers:
        - name: redis-exporter
          image: oliver006/redis_exporter:latest
          env:
            - name: REDIS_ADDR
              value: "redis://redis.ongi-data.svc.cluster.local:6379"
          ports:
            - containerPort: 9121
          resources:
            requests:
              cpu: "20m"
              memory: "64Mi"
            limits:
              cpu: "200m"
              memory: "256Mi"
---
apiVersion: v1
kind: Service
metadata:
  name: redis-exporter
  namespace: ongi-data
spec:
  selector:
    app: redis-exporter
  ports:
    - name: http
      port: 9121
      targetPort: 9121
---
# -------------------------
# Kafka (apache/kafka:4.1.1) + JMX exporter
# - 기존 compose의 server.properties / kafka.yml 반영
# - jmx_prometheus_javaagent.jar는 initContainer에서 다운로드
# -------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-config
  namespace: ongi-data
data:
  server.properties: |
    process.roles=broker,controller
    node.id=1
    listeners=INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:29092,CONTROLLER://0.0.0.0:9093
    advertised.listeners=INTERNAL://kafka.ongi-data.svc.cluster.local:9092,EXTERNAL://localhost:29092
    listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
    inter.broker.listener.name=INTERNAL
    controller.listener.names=CONTROLLER
    controller.quorum.voters=1@kafka.ongi-data.svc.cluster.local:9093
    log.dirs=/var/lib/kafka/data
    num.network.threads=3
    num.io.threads=8
    background.threads=4
    socket.send.buffer.bytes=1048576
    socket.receive.buffer.bytes=1048576
    log.segment.bytes=1073741824
    log.retention.hours=72
    log.retention.check.interval.ms=300000
    log.cleanup.policy=delete
    auto.create.topics.enable=true
    num.partitions=3
    default.replication.factor=1
    offsets.topic.replication.factor=1
    transaction.state.log.replication.factor=1
    transaction.state.log.min.isr=1
    group.initial.rebalance.delay.ms=0

  kafka.yml: |
    startDelaySeconds: 0
    ssl: false
    lowercaseOutputName: true
    lowercaseOutputLabelNames: true
    whitelistObjectNames:
      - "kafka.network:*"
      - "kafka.server:*"
      - "kafka.log:*"
      - "kafka.cluster:*"
      - "kafka.controller:*"
      - "kafka.coordinator.group:*"
      - "kafka.coordinator.transaction:*"
      - "org.apache.kafka.server:*"
    rules:
      # ---------------------------------------------------------------------------
      # kafka.network - RequestMetrics
      # canonical: kafka.network<type=RequestMetrics, name=..., request=... [, error=...]><>Attr
      # ---------------------------------------------------------------------------
      - pattern: 'kafka\.network<type=RequestMetrics,\s*name=RequestsPerSec,\s*request=([^,>]+)><>Count'
        name: kafka_request_total
        labels:
          request: "$1"
      - pattern: 'kafka\.network<type=RequestMetrics,\s*name=RequestsPerSec,\s*request=([^,>]+)><>OneMinuteRate'
        name: kafka_request_rate_1m
        labels:
          request: "$1"
      # Latency quantiles (p50/p95/p99 ...)
      - pattern: 'kafka\.network<type=RequestMetrics,\s*name=(LocalTimeMs|RemoteTimeMs|TotalTimeMs|RequestQueueTimeMs|ResponseQueueTimeMs|ThrottleTimeMs),\s*request=([^,>]+)><>p(\d+)'
        name: kafka_request_$1_ms
        labels:
          request: "$2"
          quantile: "p$3"
      # ErrorsPerSec (error label)
      - pattern: 'kafka\.network<type=RequestMetrics,\s*name=ErrorsPerSec,\s*request=([^,>]+),\s*error=([^,>]+)><>Count'
        name: kafka_request_errors_total
        labels:
          request: "$1"
          error: "$2"
      - pattern: 'kafka\.network<type=RequestMetrics,\s*name=ErrorsPerSec,\s*request=([^,>]+),\s*error=([^,>]+)><>OneMinuteRate'
        name: kafka_request_errors_rate_1m
        labels:
          request: "$1"
          error: "$2"
      # ---------------------------------------------------------------------------
      # kafka.network - Processor / SocketServer / Acceptor
      # ---------------------------------------------------------------------------
      - pattern: 'kafka\.network<type=Processor,\s*name=IdlePercent,\s*networkProcessor=(\d+)><>Value'
        name: kafka_network_processor_idle_percent
        labels:
          network_processor: "$1"
      - pattern: 'kafka\.network<type=SocketServer,\s*name=NetworkProcessorAvgIdlePercent><>Value'
        name: kafka_network_processor_avg_idle_percent
      - pattern: 'kafka\.network<type=SocketServer,\s*name=MemoryPoolAvailable><>Value'
        name: kafka_socketserver_memory_pool_available_bytes
      - pattern: 'kafka\.network<type=SocketServer,\s*name=MemoryPoolUsed><>Value'
        name: kafka_socketserver_memory_pool_used_bytes
      - pattern: 'kafka\.network<type=Acceptor,\s*name=AcceptorBlockedPercent,\s*listener=([^,>]+)><>Count'
        name: kafka_acceptor_blocked_percent_total
        labels:
          listener: "$1"
      # ---------------------------------------------------------------------------
      # kafka.server - BrokerTopicMetrics / ReplicaManager / RequestHandler
      # canonical example from your dump has no spaces in the mbeans list,
      # but exporter canonical does use spaces. So we accept \s*.
      # ---------------------------------------------------------------------------
      - pattern: 'kafka\.server<type=ReplicaManager,\s*name=UnderReplicatedPartitions><>Value'
        name: kafka_under_replicated_partitions
      - pattern: 'kafka\.server<type=ReplicaManager,\s*name=AtMinIsrPartitionCount><>Value'
        name: kafka_at_min_isr_partition_count
      - pattern: 'kafka\.server<type=ReplicaManager,\s*name=(IsrExpandsPerSec|IsrShrinksPerSec)><>Count'
        name: kafka_replica_manager_$1_total
      - pattern: 'kafka\.server<type=ReplicaManager,\s*name=(IsrExpandsPerSec|IsrShrinksPerSec)><>OneMinuteRate'
        name: kafka_replica_manager_$1_rate_1m
      - pattern: 'kafka\.server<type=KafkaRequestHandlerPool,\s*name=RequestHandlerAvgIdlePercent><>Value'
        name: kafka_request_handler_avg_idle_percent
      # Broker-wide throughput
      - pattern: 'kafka\.server<type=BrokerTopicMetrics,\s*name=(BytesInPerSec|BytesOutPerSec|BytesRejectedPerSec|MessagesInPerSec|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec)><>Count'
        name: kafka_broker_$1_total
      - pattern: 'kafka\.server<type=BrokerTopicMetrics,\s*name=(BytesInPerSec|BytesOutPerSec|BytesRejectedPerSec|MessagesInPerSec|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec)><>OneMinuteRate'
        name: kafka_broker_$1_rate_1m
      # Topic-level (optional but useful in dev)
      - pattern: 'kafka\.server<type=BrokerTopicMetrics,\s*name=(BytesInPerSec|BytesOutPerSec|MessagesInPerSec),\s*topic=([^,>]+)><>OneMinuteRate'
        name: kafka_topic_$1_rate_1m
        labels:
          topic: "$2"
      # ---------------------------------------------------------------------------
      # controller / coordinator
      # ---------------------------------------------------------------------------
      - pattern: 'kafka\.controller<type=KafkaController,\s*name=ActiveControllerCount><>Value'
        name: kafka_active_controller
      - pattern: 'kafka\.controller<type=KafkaController,\s*name=OfflinePartitionsCount><>Value'
        name: kafka_offline_partitions
      - pattern: 'kafka\.coordinator\.group<type=GroupCoordinator,\s*name=NumGroups><>Value'
        name: kafka_group_coordinator_num_groups
      - pattern: 'kafka\.coordinator\.group<type=GroupCoordinator,\s*name=NumOffsets><>Value'
        name: kafka_group_coordinator_num_offsets
      - pattern: 'kafka\.coordinator\.transaction<type=TransactionCoordinator,\s*name=NumTransactions><>Value'
        name: kafka_transaction_coordinator_num_transactions
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kafka-data
  namespace: ongi-data
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 10Gi
  storageClassName: local-path
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka
  namespace: ongi-data
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
        - name: kafka
          #image: apache/kafka:4.1.1
          image: ongi-kafka:4.1.1-jmx
          ports:
            - containerPort: 9092
            - containerPort: 9093
            - containerPort: 29092
            - containerPort: 9404
          env:
            - name: KAFKA_OPTS
              value: "-javaagent:/opt/kafka/jmx/jmx_prometheus_javaagent-1.5.0.jar=9404:/opt/kafka/jmx/kafka.yml"
            - name: KAFKA_JMX_OPTS
              value: "-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.rmi.port=9999 -Djava.rmi.server.hostname=kafka"
          command:
            - sh
            - -c
            - |
              set -e

              DATA_DIR="/var/lib/kafka/data"
              META="$DATA_DIR/meta.properties"

              if [ -f "$META" ]; then
                echo "meta.properties exists; skip format"
              else
                CLUSTER_ID="$(/opt/kafka/bin/kafka-storage.sh random-uuid)"
                echo "Generated CLUSTER_ID=${CLUSTER_ID}"

                /opt/kafka/bin/kafka-storage.sh format \
                  --config /opt/kafka/config/server.properties \
                  --cluster-id "${CLUSTER_ID}" \
                  --ignore-formatted
              fi

              exec /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties
          volumeMounts:
            - name: kafka-config
              mountPath: /opt/kafka/config/server.properties
              subPath: server.properties
            - name: kafka-data
              mountPath: /var/lib/kafka/data
          resources:
            requests:
              cpu: "200m"
              memory: "512Mi"
            limits:
              cpu: "2000m"
              memory: "2Gi"
      volumes:
        - name: kafka-config
          configMap:
            name: kafka-config
        - name: kafka-data
          persistentVolumeClaim:
            claimName: kafka-data
---
apiVersion: v1
kind: Service
metadata:
  name: kafka
  namespace: ongi-data
spec:
  selector:
    app: kafka
  ports:
    - name: internal
      port: 9092
      targetPort: 9092
    - name: external
      port: 29092
      targetPort: 29092
    - name: controller
      port: 9093
      targetPort: 9093
---
# -------------------------
# Kafka Exporter (danielqsj/kafka-exporter)
# -------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka-exporter
  namespace: ongi-data
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-exporter
  template:
    metadata:
      labels:
        app: kafka-exporter
    spec:
      containers:
        - name: kafka-exporter
          image: danielqsj/kafka-exporter:latest
          args:
            - --kafka.server=kafka.ongi-data.svc.cluster.local:9092
          ports:
            - containerPort: 9308
          resources:
            requests:
              cpu: "50m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
---
apiVersion: v1
kind: Service
metadata:
  name: kafka-exporter
  namespace: ongi-data
spec:
  selector:
    app: kafka-exporter
  ports:
    - name: http
      port: 9308
      targetPort: 9308